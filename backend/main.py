import os
import re
import sys
import logging
import json
import uuid
import time
import requests
from datetime import datetime
from typing import Optional, List, Any, Dict
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv

# [ë¡œì»¬ ê°œë°œìš©] ìƒìœ„ í´ë”(.env)ì˜ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# Dockerì—ì„œëŠ” env_fileë¡œ ì£¼ì…ë˜ì§€ë§Œ, ë¡œì»¬ ì‹¤í–‰ ì‹œ í•„ìš”í•©ë‹ˆë‹¤.
load_dotenv(dotenv_path="../.env")

# --- [Monkey Patching: í˜¸í™˜ì„± í•´ê²°] ---
import langchain_core.callbacks
import langchain_core.callbacks.base
import langchain_core.agents
import langchain_core.documents
import langchain_core.messages
import langchain_core.outputs

sys.modules["langchain.callbacks"] = langchain_core.callbacks
sys.modules["langchain.callbacks.base"] = langchain_core.callbacks.base
sys.modules["langchain.schema"] = langchain_core.messages 
sys.modules["langchain.schema.agent"] = langchain_core.agents
sys.modules["langchain.schema.document"] = langchain_core.documents

# [Langfuse & LangChain Integrations]
from langchain_core.messages import SystemMessage, HumanMessage
from langfuse.decorators import observe, langfuse_context
from langfuse.callback import CallbackHandler

# --- [Biomni Import] ---
from biomni.agent.a1 import A1

load_dotenv(dotenv_path="../.env")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("BiomniWeb")

app = FastAPI()

# 1. ì •ì  íŒŒì¼ ì„œë¹™ ì¶”ê°€ (ìƒì„±ëœ ì´ë¯¸ì§€/PDF ì ‘ê·¼ìš©)
# Docker volume ë§¤í•‘ì— ë§ì¶° ê²½ë¡œ ì„¤ì • (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ: /app/data)
if os.path.exists("/app/data"):
    app.mount("/data", StaticFiles(directory="/app/data"), name="data")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

agent: Optional[A1] = None

def initialize_agent():
    global agent
    logger.info("Initializing Biomni Agent...")
    try:
        # Docker í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ë¡œì»¬ ê²½ë¡œ
        data_path = os.getenv("BIOMNI_DATA_PATH", "../biomni_data")
        agent = A1(path=data_path)
        logger.info("Biomni Agent initialized successfully.")
    except Exception as e:
        logger.error(f"Failed to initialize Biomni Agent: {e}")
        agent = None

initialize_agent()

class ChatRequest(BaseModel):
    message: str

# ì‘ë‹µ ëª¨ë¸ ì •ì˜ (ë¡œê·¸ í¬í•¨)
class ChatResponse(BaseModel):
    response: str
    logs: List[Any]  # Biomniì˜ response_log êµ¬ì¡°ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì„¤ì •
    refined_data: Dict[str, Any] = {}

@app.post("/api/chat", response_model=ChatResponse)
@observe(name="Biomni Chat Interaction")
async def chat_endpoint(request: ChatRequest):
    if not agent:
        # ì¬ì‹œë„ ë¡œì§ (í•„ìš”ì‹œ)
        initialize_agent()
        if not agent:
            raise HTTPException(status_code=500, detail="Agent not initialized")
    
    logger.info(f"Received request: {request.message}")
    
    try:
        # ... (ê¸°ì¡´ Monkey Patching ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€) ...
        # (original_stream, traced_stream ì •ì˜ ë¶€ë¶„)

        # agent.app.stream = traced_stream 
        
        # 4. ì—ì´ì „íŠ¸ ì‹¤í–‰
        langfuse_handler = langfuse_context.get_current_langchain_handler()
        response_log, response_content = agent.go(request.message, callbacks=[langfuse_handler])
        
        # agent.app.stream = original_stream
        
        # Langfuse ì—…ë°ì´íŠ¸
        langfuse_context.update_current_trace(
            output=str(response_content),
            metadata={"full_log_length": len(response_log)}
        )

        # ğŸŒŸ ë§¤ìš° ì¤‘ìš”: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜ì§‘ ì¤‘ì¸ ëª¨ë“  í•˜ìœ„ Spanë“¤ì„ ì„œë²„ë¡œ ê°•ì œ ì „ì†¡ ì™„ë£Œì‹œí‚µë‹ˆë‹¤.
        langfuse_context.flush()

        trace_id = langfuse_context.get_current_trace_id()
        
        # Langfuse ì„œë²„ì˜ DBì— í•˜ìœ„ Spanë“¤ì´ ì™„ì „íˆ ê¸°ë¡ë  ë•Œê¹Œì§€ ì•„ì£¼ ì ê¹(1~2ì´ˆ) ëŒ€ê¸°í•©ë‹ˆë‹¤.
        time.sleep(3) 
        
        langfuse_host = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com").rstrip("/")
        public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
        secret_key = os.getenv("LANGFUSE_SECRET_KEY")
        
        # Langfuse REST APIë¥¼ í˜¸ì¶œí•˜ì—¬ í•´ë‹¹ íŠ¸ë ˆì´ìŠ¤ì˜ 'ëª¨ë“  ì„¸ë¶€ ì •ë³´'ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
        api_url = f"{langfuse_host}/api/public/traces/{trace_id}"
        api_response = requests.get(api_url, auth=(public_key, secret_key))
        
        if api_response.status_code == 200:
            full_trace_data = api_response.json()
            
            base_log_dir = "/app/logs"
            
            # ==============================================================
            # 1. ì›ë³¸ ë°ì´í„° (Raw Data) ì €ì¥
            # ==============================================================
            raw_dir = os.path.join(base_log_dir, "raw")
            os.makedirs(raw_dir, exist_ok=True)
            
            raw_data = {
                "trace_id": trace_id,
                "timestamp": datetime.now().isoformat(),
                "instruction": request.message,
                "langfuse_full_trace": full_trace_data, 
                "response_log": response_log,
                "final_answer": str(response_content)
            }
            
            # raw_file_path = os.path.join(raw_dir, f"trace_{trace_id}.json")
            # with open(raw_file_path, "w", encoding="utf-8") as f:
            #     json.dump(raw_data, f, ensure_ascii=False, indent=4)
                
            # ==============================================================
            # 2. íŒŒì¸íŠœë‹ìš© ì •ì œ ë°ì´í„° (Refined Data) - Langfuse Observation ì§ì ‘ íŒŒì‹±
            # ==============================================================
            
            refined_dir = os.path.join(base_log_dir, "refined")
            os.makedirs(refined_dir, exist_ok=True)
            
            def clean_think(text):
                if not text: return ""
                text = re.sub(r"<think>", "", text)
                text = re.sub(r"</think>", "", text)
                return text.strip()

            # ğŸŒŸ [ì‹ ê·œ] Trace ë°ì´í„° ì „ì²´ë¥¼ ë’¤ì ¸ì„œ 'ì›ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸'ë¥¼ ë™ì ìœ¼ë¡œ ì™„ë²½ ì¶”ì¶œí•©ë‹ˆë‹¤ (í•˜ë“œì½”ë”© ì œë¡œ)
            def find_system_content(obj):
                if isinstance(obj, dict):
                    if obj.get("type") in ["system", "system_message"] and "content" in obj:
                        return str(obj["content"])
                    if isinstance(obj.get("id"), list) and obj.get("id") and obj.get("id")[-1] == "SystemMessage":
                        return str(obj.get("kwargs", {}).get("content", ""))
                    if obj.get("role") == "system" and "content" in obj:
                        return str(obj["content"])
                    for k, v in obj.items():
                        res = find_system_content(v)
                        if res: return res
                elif isinstance(obj, (list, tuple)):
                    for item in obj:
                        res = find_system_content(item)
                        if res: return res
                return ""

            messages = []
            
            # 1. System Message ë™ì  ì„¸íŒ…
            extracted_sys_content = find_system_content(full_trace_data)
            if extracted_sys_content:
                # ì¶”ì¶œëœ ì›ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì“°ê³ , R1 íŒŒì¸íŠœë‹ìš© <think> ì§€ì‹œë¬¸ë§Œ ëì— ì¶”ê°€
                if "<think>" not in extracted_sys_content:
                    sys_content = extracted_sys_content + "\n\nUse <think>...</think> tags to show your step-by-step reasoning process before acting."
                else:
                    sys_content = extracted_sys_content
            else:
                # ë§Œì•½ì˜ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ìµœì†Œí•œì˜ Fallback (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
                sys_content = "You are Biomni, an advanced reasoning and acting agent. Use <think>...</think> tags to show your step-by-step reasoning process before acting."

            messages.append({
                "id": None, "name": None, "type": "system", "content": sys_content,
                "additional_kwargs": {}, "response_metadata": {}
            })
            
            # 2. Human Message ì„¸íŒ…
            messages.append({
                "id": None, "name": None, "type": "human", "content": request.message,
                "additional_kwargs": {}, "response_metadata": {}
            })

            # 3. Langfuse REST APIë¡œ ë°›ì•„ì˜¨ í™•ì • ë°ì´í„°ë¥¼ íŒŒì‹± (ë©”ëª¨ë¦¬ íœ˜ë°œ ë°©ì§€)
            observations = full_trace_data.get("observations", [])
            observations.sort(key=lambda x: x.get("startTime", "")) 
            
            think_steps = []
            seen_contents = set()
            clean_final = clean_think(str(response_content)).strip()
            
            # ğŸŒŸ [ì¶”ê°€ëœ ë¶€ë¶„] ë‘ ê°€ì§€ ë‚´ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            llm_thoughts = [] 
            tool_results = []
            
            for obs in observations:
                obs_type = obs.get("type")
                name = obs.get("name", "")
                
                if name == "Biomni Chat Interaction":
                    continue
                    
                output = obs.get("output")
                if not output:
                    continue
                    
                content = ""
                if isinstance(output, dict):
                    if "kwargs" in output and "content" in output["kwargs"]:
                        content = str(output["kwargs"].get("content", ""))
                    elif "content" in output:
                        content = str(output.get("content", ""))
                    else:
                        content = json.dumps(output, ensure_ascii=False)
                else:
                    content = str(output)
                    
                clean_content = clean_think(content)
                if not clean_content.strip():
                    continue
                    
                if clean_content in seen_contents:
                    continue
                seen_contents.add(clean_content)
                
                if clean_content == clean_final:
                    continue
                
                # ğŸŒŸ [ìˆ˜ì •ëœ ë¶€ë¶„] GENERATION(ìƒê°)ê³¼ SPAN(ë„êµ¬ ì‹¤í–‰) ë¶„ë¦¬ ë° ë³„ë„ ì €ì¥
                if obs_type == "GENERATION":
                    think_steps.append(clean_content)
                    llm_thoughts.append(clean_content)  # LLMì˜ ìƒê° ê³¼ì • ì €ì¥
                    
                elif obs_type == "SPAN":
                    # ë„êµ¬ì— ë“¤ì–´ê°„ ì…ë ¥ê°’(Input)ë„ í•¨ê»˜ ì¶”ì¶œ (ì¶”ì ì— ë§¤ìš° ìœ ìš©í•¨)
                    obs_input = obs.get("input", "")
                    
                    # Tool ì‹¤í–‰ ë‚´ì—­ ì €ì¥ (ì´ë¦„, ì…ë ¥ê°’, ê²°ê³¼ê°’)
                    tool_results.append({
                        "tool_name": name,
                        "tool_input": obs_input,
                        "tool_output": clean_content
                    })
                    
                    if "feedback" in name.lower() or "error" in name.lower():
                        think_steps.append(f"System Feedback:\n{clean_content}")
                    else:
                        # R1 ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ì—ë„ Tool ì´ë¦„ì´ ëª…ì‹œë˜ë„ë¡ ê°œì„ 
                        think_steps.append(f"Action (Tool: {name}):\nInput: {obs_input}\nObservation:\n{clean_content}")

            # 4. DeepSeek R1 ìŠ¤íƒ€ì¼ ì¡°ë¦½
            combined_think = "\n\n".join(think_steps)
            if combined_think.strip():
                deepseek_content = f"<think>\n{combined_think}\n</think>\n\n{clean_final}"
            else:
                deepseek_content = clean_final
                
            messages.append({
                "id": None, "name": None, "type": "ai", "content": deepseek_content,
                "tool_calls": [], "invalid_tool_calls": [], "usage_metadata": None,
                "additional_kwargs": {}, "response_metadata": {}
            })

            # ğŸŒŸ [ìˆ˜ì •ëœ ë¶€ë¶„] refined_data êµ¬ì¡°ì— ì¶”ì¶œí•œ ë‘ ê°€ì§€ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            refined_data = {
                "trace_id": trace_id,
                "final_answer": str(response_content),
                "messages": messages,
                "llm_thoughts": llm_thoughts,  # LLMì´ ìƒê°í•œ ê³¼ì • ë°°ì—´
                "tool_results": tool_results   # ê° ë„êµ¬ì˜ ì´ë¦„/ì…ë ¥/ê²°ê³¼ ë°°ì—´
            }
            
            # (ì„ íƒ ì‚¬í•­) ë§Œì•½ ì´ ì •ì œëœ ë°ì´í„°ë¥¼ ì‹¤ì œ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ 
            # ì•„ë˜ ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œë¥¼ í•´ì œ(Uncomment)í•´ ì£¼ì„¸ìš”.
            # refined_file_path = os.path.join(refined_dir, f"trace_{trace_id}.json")
            # with open(refined_file_path, "w", encoding="utf-8") as f:
            #     json.dump(refined_data, f, ensure_ascii=False, indent=4)
                
            logger.info(f"âœ… Saved raw and refined trace data (DeepSeek R1 style) to {base_log_dir}")

        # 5. [ìˆ˜ì •ë¨] ë¡œê·¸ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜
        return {
            "response": str(response_content),
            "logs": response_log, # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì¤‘ê°„ ê³¼ì •ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ í•„ìˆ˜
            "refined_data": refined_data if 'refined_data' in locals() else {}
        }
    
    except Exception as e:
        logger.error(f"Error during execution: {e}")
        langfuse_context.update_current_trace(level="ERROR", status_message=str(e))
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    # app ëŒ€ì‹  "main:app" ë¬¸ìì—´ë¡œ ë„£ê³ , reload=Trueë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)